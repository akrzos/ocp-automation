---
- name: set workdir
  set_fact:
    workdir: "{{ GOPATH }}/src/github.com/openshift/installer"

- name: vars path
  set_fact:
    openshift_vars_file: "{{ansible_env.HOME}}/ocp-env.sh"

- name: ensure that .kube dir exists
  file:
    path: /root/.kube
    state: directory

    #- name: masters
    #shell: oc get nodes | grep master | awk '{print $1}'
    #register: ocp_masters

- name: copy the kubeconfig
  copy:
    src: "{{ workdir }}/auth/kubeconfig"
    dest: /root/.kube/config

- name: stat svt repo
  stat:
    path: /root/svt
  register: svt

- name: clone svt repo
  git:
    repo: https://github.com/openshift/svt
    dest: /root/svt
    update: yes
  when: svt.stat.exists == False

- name: pick one master node
  shell: oc get nodes -l node-role.kubernetes.io/master | awk 'NR>1 {print $1}' | tail -n1
  register: master_node

- name: pick one worker node
  shell: oc get nodes -l node-role.kubernetes.io/worker | awk 'NR>1 {print $1}' | tail -n1
  register: controller_node

- name: remove the label on the worker node
  shell: oc label node {{ controller_node.stdout }} node-role.kubernetes.io/worker-

- name: add the controller label
  shell: oc label node {{ controller_node.stdout }} {{ item }} --overwrite=true
  with_items:
    - "node-role.kubernetes.io/controller=true"
    - "role=controller"

- name: add a taint to the controller
  shell: oc adm taint node {{ controller_node.stdout }} role=controller:NoSchedule

- name: controller node
  debug:
    msg: "The controller node is {{ controller_node.stdout }}"

- block:
    - name: get the public ip of the master
      shell: aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,PrivateIpAddress,PublicIpAddress, PrivateDnsName]' --output text | column -t | grep {{ OPENSHIFT_INSTALL_CLUSTER_NAME }} | grep {{ master_node.stdout }} | awk '{print $5}'
      register: master_public_ip
    
    - name: get the private ip of the controller
      shell: aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,PrivateIpAddress,PublicIpAddress, PrivateDnsName]' --output text | column -t | grep {{ OPENSHIFT_INSTALL_CLUSTER_NAME }} | grep {{ controller_node.stdout }} | awk '{print $6}'
      register: controller_private_ip
    
    - name: add controller to group         i      
      add_host: name={{ controller_private_ip.stdout }} groups=controller

    - name: set controller host as fact
      set_fact:
        controller: "{{ controller_private_ip.stdout }}"

    - name: get VpcId
      shell: aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,PrivateIpAddress,PublicIpAddress, PrivateDnsName, VpcId]' --output text | column -t | grep {{ OPENSHIFT_INSTALL_CLUSTER_NAME }} | awk '{print $7}' | grep -v '^$' | sort -u
      register: vpc_id

    - name: get security groups
      shell: aws ec2 describe-security-groups --filters "Name=vpc-id,Values={{ vpc_id.stdout }}" --output json | jq .SecurityGroups[].GroupId
      register: security_groups

    - name: add inbound rule to allow traffic on 2022
      shell: aws ec2 authorize-security-group-ingress --group-id {{ item }} --protocol tcp --port 2022 --cidr 0.0.0.0/0
      with_items:
        - "{{ security_groups.stdout_lines }}"
      ignore_errors: yes

    - name: ssh config for nodes
      template:
        src: ssh-config.j2
        dest: /root/.ssh/config
      delegate_to: localhost
  when: OPENSHIFT_INSTALL_PLATFORM == "aws"
