---
- name: set workdir
  set_fact:
    workdir: "{{ GOPATH }}/src/github.com/openshift/installer"

- name: vars path
  set_fact:
    openshift_vars_file: "{{ansible_env.HOME}}/ocp-env.sh"

- name: ensure that .kube dir exists
  file:
    path: /root/.kube
    state: directory

    #- name: masters
    #shell: oc get nodes | grep master | awk '{print $1}'
    #register: ocp_masters

- name: copy the kubeconfig
  copy:
    src: "{{ workdir }}/auth/kubeconfig"
    dest: /root/.kube/config

- name: stat svt repo
  stat:
    path: /root/svt
  register: svt

- name: clone svt repo
  git:
    repo: https://github.com/openshift/svt
    dest: /root/svt
    update: yes
    version: containerozed_tooling
  when: svt.stat.exists == False

- name: pick one master node
  shell: oc get nodes -l node-role.kubernetes.io/master | awk 'NR>1 {print $1}' | tail -n1
  register: master_node

- name: Get cluster name
  shell: |
    {%raw%}oc get machineset -n openshift-cluster-api -o=go-template='{{(index (index .items 0).metadata.labels "sigs.k8s.io/cluster-api-cluster")}}'{%endraw%}
  register: rhcos_cluster_name

- name: Get cluster ID
  shell: |
    {%raw%}oc get machineset -n openshift-cluster-api -o=go-template='{{with (index .items 0)}}{{range .spec.template.spec.providerSpec.value.tags}}{{if eq .name "openshiftClusterID"}}{{.value}}{{end}}{{end}}{{end}}'{%endraw%}
  register: rhcos_cluster_id

- name: Get AMI ID
  shell: |
    {%raw%}oc get machineset -n openshift-cluster-api -o=go-template='{{(index .items 0).spec.template.spec.providerSpec.value.ami.id}}'{%endraw%}
  register: rhcos_ami_id

- name: Get cluster region
  shell: |
    {%raw%}oc get machineset -n openshift-cluster-api -o=go-template='{{(index .items 0).spec.template.spec.providerSpec.value.placement.region}}'{%endraw%}
  register: rhcos_region

- name: Place machineset yamls on master
  template:
    src: "{{item.src}}"
    dest: "{{item.dest}}"
  with_items:
    - src: templates/infra-node-machineset.yml.j2
      dest: infra-node-machineset.yml
    - src: templates/pbench-node-machineset.yml.j2
      dest: pbench-node-machineset.yml

- name: Get current ready node count
  shell: oc get nodes | grep " Ready" -ic
  register: rhcos_current_node_count

- name: Create machinesets
  shell: |
    oc create -f infra-node-machineset.yml,pbench-node-machineset.yml

- name: Poll nodes to see if creating nodes finished
  shell: oc get nodes | grep " Ready" -ic
  register: current_node_count
   # + 3 (infra nodes) + 1 (pbench controller node)
  until: current_node_count.stdout|int == (rhcos_current_node_count.stdout|int + 3 + 1)
  delay: 1
  retries: "{{poll_attempts|int}}"

- name: Relabel the infra nodes
  shell: |
    oc label nodes --overwrite -l 'node-role.kubernetes.io/infra=' node-role.kubernetes.io/worker-
  # oc label nodes --overwrite -l 'beta.kubernetes.io/instance-type={{rhcos_infra_node_instance_type}}' node-role.kubernetes.io/worker- node-role.kubernetes.io/infra=""

- name: Relabel the pbench controller node
  shell: |
    oc label nodes --overwrite -l 'node-role.kubernetes.io/pbench=' node-role.kubernetes.io/worker-
  # oc label nodes --overwrite -l 'beta.kubernetes.io/instance-type={{rhcos_pbench_node_instance_type}}' node-role.kubernetes.io/worker- node-role.kubernetes.io/pbench=""

- name: Disable CVO to prevent squashed configuration changes to cluster operators
  shell: |
    oc scale --replicas 0 -n openshift-cluster-version deployments/cluster-version-operator

- name: Taint the pbench controller node
  shell: |
    oc adm taint node -l node-role.kubernetes.io/pbench= dedicated=pbench:NoSchedule

- name: Copy new cluster-monitoring-config
  copy:
    src: files/cluster-monitoring-config.yml
    dest: cluster-monitoring-config.yml

- name: Replace the cluster-monitoring-config ConfigMap
  shell: |
    oc replace -f cluster-monitoring-config.yml

# Ingress comes with a nodeSelector for worker nodes
- name: Remove existing nodeSelector from ingress
  shell: |
    oc patch deployment.apps/router-default -n openshift-ingress --type json -p '[{"op": "remove", "path": "/spec/template/spec/nodeSelector"}]'

- name: Apply new node-selector to infra workload components
  shell: |
    oc patch {{item.object}} -n {{item.namespace}} -p '{"spec": {"template": {"spec": {"nodeSelector": {"node-role.kubernetes.io/infra": ""}}}}}'
  with_items:
    # Monitoring
    - namespace: openshift-monitoring
      object: deployment.apps/cluster-monitoring-operator
    - namespace: openshift-monitoring
      object: deployment.apps/prometheus-adapter
    # Registry
    - namespace: openshift-image-registry
      object: deployment.apps/cluster-image-registry-operator
    - namespace: openshift-image-registry
      object: deployment.apps/image-registry
    # Ingress (Router)
    - namespace: openshift-ingress-operator
      object: deployment.apps/ingress-operator
    - namespace: openshift-ingress
      object: deployment.apps/router-default

- name: Relabel the pbench controller node
  shell: |
    oc label nodes --overwrite -l 'beta.kubernetes.io/instance-type={{rhcos_pbench_node_instance_type}}' node-role.kubernetes.io/worker- node-role.kubernetes.io/pbench=""

- name: get the controller node
  shell: oc get nodes -l node-role.kubernetes.io/pbench="" | awk 'NR>1 {print $1}' | tail -n1
  register: controller_node

- name: add the controller label
  shell: oc label node {{ controller_node.stdout }} {{ item }} --overwrite=true
  with_items:
    - "role=controller"

- name: add a taint to the controller
  shell: oc adm taint node {{ controller_node.stdout }} role=controller:NoSchedule

- block:
    - name: get the public ip of the master
      shell: aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,PrivateIpAddress,PublicIpAddress, PrivateDnsName]' --output text | column -t | grep {{ OPENSHIFT_INSTALL_CLUSTER_NAME }} | grep {{ master_node.stdout }} | awk '{print $5}'
      register: master_public_ip
    
    - name: get the private ip of the controller
      shell: aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,PrivateIpAddress,PublicIpAddress, PrivateDnsName]' --output text | column -t | grep {{ OPENSHIFT_INSTALL_CLUSTER_NAME }} | grep {{ controller_node.stdout }} | awk '{print $6}'
      register: controller_private_ip
    
    - name: add controller to group         i      
      add_host: name={{ controller_private_ip.stdout }} groups=controller

    - name: set controller host as fact
      set_fact:
        controller: "{{ controller_private_ip.stdout }}"

    - name: get VpcId
      shell: aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,PrivateIpAddress,PublicIpAddress, PrivateDnsName, VpcId]' --output text | column -t | grep {{ OPENSHIFT_INSTALL_CLUSTER_NAME }} | awk '{print $7}' | grep -v '^$' | sort -u
      register: vpc_id

    - name: get security groups
      shell: aws ec2 describe-security-groups --filters "Name=vpc-id,Values={{ vpc_id.stdout }}" --output json | jq .SecurityGroups[].GroupId
      register: security_groups

    - name: add inbound rule to allow traffic on 2022
      shell: aws ec2 authorize-security-group-ingress --group-id {{ item }} --protocol tcp --port 2022 --cidr 0.0.0.0/0
      with_items:
        - "{{ security_groups.stdout_lines }}"
      ignore_errors: yes

    - name: ssh config for nodes
      template:
        src: ssh-config.j2
        dest: /root/.ssh/config
      delegate_to: localhost
  when: OPENSHIFT_INSTALL_PLATFORM == "aws"
